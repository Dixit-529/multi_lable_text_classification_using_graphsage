{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phPvDF_WTeH9"
      },
      "source": [
        "#Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoPU5EwJdlr4",
        "outputId": "d1a9a444-144c-45ff-fd30-2802fc935d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeHfYDNGoGka",
        "outputId": "a9f9f578-91b0-47a9-a51a-f65bc0c1cc2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import reuters\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import PorterStemmer, WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import hamming_loss, f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import pickle\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O0stJr-VL1h8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xLEwWKB6QtL"
      },
      "source": [
        "#Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9O8H5bRTsHPL"
      },
      "outputs": [],
      "source": [
        "#Build adjacency matrix based on Co-Occurencies label\n",
        "def buildAdjacencyCOOC(data_label):\n",
        "  adj = data_label.T.dot(data_label).astype('float')\n",
        "  for i in range(len(adj)):\n",
        "    adj[i] = adj[i] / adj[i,i]\n",
        "\n",
        "  return torch.from_numpy(adj.astype('float32'))\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#Text cleaning function\n",
        "def preprocessingText(text, stop=stop):\n",
        "  text = text.lower() #text to lowercase\n",
        "  text = re.sub(r'&lt;', '', text) #remove '&lt;' tag\n",
        "  text = re.sub(r'<.*?>', '', text) #remove html\n",
        "  text = re.sub(r'[0-9]+', '', text) #remove number\n",
        "  text = \" \".join([word for word in text.split() if word not in stop]) #remove stopwords\n",
        "  text = re.sub(r'[^\\w\\s]', '', text) #remove punctiation\n",
        "  text = re.sub(r'[^\\x00-\\x7f]', '', text) #remove non ASCII strings\n",
        "  for c in ['\\r', '\\n', '\\t'] :\n",
        "    text = re.sub(c, ' ', text) #replace newline and tab with tabs\n",
        "  text = re.sub('\\s+', ' ', text) #replace multiple spaces with one space\n",
        "  text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "  return text\n",
        "\n",
        "#Load Word Representation Vector\n",
        "def loadWRVModel(File):\n",
        "    print(\"Loading Word Representation Vector Model\")\n",
        "    f = open(File,'r')\n",
        "    WRVModel = {}\n",
        "    for line in f:\n",
        "        splitLines = line.split()\n",
        "        word = splitLines[0]\n",
        "        try:\n",
        "          wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
        "        except:\n",
        "          print(splitLines[1:])\n",
        "          print(len(splitLines[1:]))\n",
        "          break\n",
        "        WRVModel[word] = wordEmbedding\n",
        "    print(len(WRVModel),\" words loaded!\")\n",
        "    return WRVModel\n",
        "\n",
        "\n",
        "def check_accuracy(model, label_embedding, X, y):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out = model(X, label_embedding)\n",
        "    y_pred = torch.sigmoid(out.detach()).round().cpu()\n",
        "    f1score = f1_score(y, y_pred, average='micro')\n",
        "    hammingloss = hamming_loss(y, y_pred)\n",
        "\n",
        "  return hammingloss, f1score\n",
        "\n",
        "def train(model,\n",
        "          X_train,\n",
        "          X_test,\n",
        "          label_embedding,\n",
        "          y_train,\n",
        "          y_test,\n",
        "          total_epoch=250,\n",
        "          batch_size=250,\n",
        "          learning_rate=0.001,\n",
        "          save_path='./model.pt',\n",
        "          state=None):\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  train_data = DataLoader(dataset(X_train, y_train), batch_size=batch_size)\n",
        "  X_test = X_test.to(device)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  label_embedding= label_embedding.to(device)\n",
        "\n",
        "  if state:\n",
        "    state = torch.load(state)\n",
        "    model = model.load_state_dict(state['last_model'])\n",
        "    optimizer = optimizer.load_state_dict(state['optimizer'])\n",
        "\n",
        "  else:\n",
        "    model = model.to(device)\n",
        "    state = dict()\n",
        "    state['microf1'] = []\n",
        "    state['hammingloss'] = []\n",
        "    state['val_hammingloss'] = []\n",
        "    state['val_microf1'] = []\n",
        "    state['epoch_time'] = []\n",
        "\n",
        "  epoch = 1\n",
        "\n",
        "  best_train = 0\n",
        "  best_val = 0\n",
        "\n",
        "  while epoch <= total_epoch:\n",
        "    running_loss = 0\n",
        "    y_pred = []\n",
        "    epoch_time = 0\n",
        "    model.train()\n",
        "    for index, (X, y) in enumerate(train_data):\n",
        "\n",
        "      t = time.time()\n",
        "\n",
        "      #forward\n",
        "      out = model(X.to(device), label_embedding)\n",
        "      loss = criterion(out, y.to(device))\n",
        "\n",
        "      #backward\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "      #update\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_time += time.time() - t\n",
        "      y_pred.append(torch.sigmoid(out.detach()).round().cpu())\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    y_pred = torch.vstack(y_pred)\n",
        "    f1score = f1_score(y_train, y_pred, average='micro')\n",
        "    hammingloss = hamming_loss(y_train, y_pred)\n",
        "    val_hamming, val_f1score = check_accuracy(model, label_embedding, X_test, y_test)\n",
        "\n",
        "    state['microf1'].append(f1score)\n",
        "    state['hammingloss'].append(hammingloss)\n",
        "    state['val_microf1'].append(val_f1score)\n",
        "    state['epoch_time'].append(epoch_time)\n",
        "    state['val_hammingloss'].append(val_hamming)\n",
        "\n",
        "    state['optimizer'] = optimizer.state_dict()\n",
        "    state['last_model'] = model.state_dict()\n",
        "\n",
        "\n",
        "    if(best_train < f1score):\n",
        "      state['model_best_train'] = copy.deepcopy(model.state_dict())\n",
        "      best_train = f1score\n",
        "      state['best_train'] = best_train\n",
        "\n",
        "    if(best_val < val_f1score):\n",
        "      state['model_best_val'] = copy.deepcopy(model.state_dict())\n",
        "      best_val = val_f1score\n",
        "      state['best_val'] = best_val\n",
        "\n",
        "    torch.save(state, save_path)\n",
        "    print('epoch:{} loss:{:.5f} hamming_loss:{:.5f} micro_f1score:{:.5f} val_hamming_loss:{:.5f} val_micro_f1score:{:.5f}'.\n",
        "          format(epoch, running_loss, hammingloss, f1score, val_hamming, val_f1score))\n",
        "    epoch+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgZvCUaKY-xZ"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1PuvFDqT2Dv"
      },
      "source": [
        "## Load Raw Dataset (Reuters-21578)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD3nYHOGiwSQ",
        "outputId": "bd99ea0a-f7fd-4bf8-a179-b8cbafc88ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train label shape torch.Size([7769, 90])\n",
            "Test label shape torch.Size([3019, 90])\n"
          ]
        }
      ],
      "source": [
        "data_train = pd.read_pickle('./train.pickle')\n",
        "data_test = pd.read_pickle('./test.pickle')\n",
        "\n",
        "text_train = data_train.text.values\n",
        "text_test = data_test.text.values\n",
        "\n",
        "y_train = torch.from_numpy(np.vstack(data_train.onehot_label.values)).float()\n",
        "y_test = torch.from_numpy(np.vstack(data_test.onehot_label.values)).float()\n",
        "\n",
        "print('Train label shape {}'.format(y_train.shape))\n",
        "print('Test label shape {}'.format(y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ-0B4vnZJpB"
      },
      "source": [
        "#Load MultilabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSJ0YUzGZTO-",
        "outputId": "f19a368c-0394-4bcd-add8-bbbb3b05c5d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MultiLabelBinarizer from version 0.22.2.post1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "with open('./multilabelbinarizer.pickle', 'rb') as file:\n",
        "  mlb = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RminIPRXkY8"
      },
      "source": [
        "#Build Word Representation Vector Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3L7jlurXpjk",
        "outputId": "eca09ff6-53f4-4fdd-c946-098f2214c226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Word Representation Vector Model\n",
            "32366  words loaded!\n"
          ]
        }
      ],
      "source": [
        "WRVModel = loadWRVModel('./glove.6B.300d.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjxasUuZ4lRi"
      },
      "source": [
        "#Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq5ref4z4oCq"
      },
      "source": [
        "## Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIAPLVCmox6A",
        "outputId": "902b9e80-11f2-4b54-b1cd-0ab3156ed578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEFORE CLEANING: BAHIA COCOA REVIEW\n",
            "  Showers continued throughout the week in\n",
            "  the Bahia cocoa zone, alleviating the drought since early\n",
            "  January and improving prospects for the coming temporao,\n",
            "  although normal humidity levels have not been restored,\n",
            "  Comissaria Smith said in its weekly review.\n",
            "      The dry period means the temporao will be late this year.\n",
            "      Arrivals for the week ended February 22 were 155,221 bags\n",
            "  of 60 kilos making a cumulative total for the season of 5.93\n",
            "  mln against 5.81 at the same stage last year. Again it seems\n",
            "  that cocoa delivered earlier on consignment was included in the\n",
            "  arrivals figures.\n",
            "      Comissaria Smith said there is still some doubt as to how\n",
            "  much old crop cocoa is still available as harvesting has\n",
            "  practically come to an end. With total Bahia crop estimates\n",
            "  around 6.4 mln bags and sales standing at almost 6.2 mln there\n",
            "  are a few hundred thousand bags still in the hands of farmers,\n",
            "  middlemen, exporters and processors.\n",
            "      There are doubts as to how much of this cocoa would be fit\n",
            "  for export as shippers are now experiencing dificulties in\n",
            "  obtaining +Bahia superior+ certificates.\n",
            "      In view of the lower quality over recent weeks farmers have\n",
            "  sold a good part of their cocoa held on consignment.\n",
            "      Comissaria Smith said spot bean prices rose to 340 to 350\n",
            "  cruzados per arroba of 15 kilos.\n",
            "      Bean shippers were reluctant to offer nearby shipment and\n",
            "  only limited sales were booked for March shipment at 1,750 to\n",
            "  1,780 dlrs per tonne to ports to be named.\n",
            "      New crop sales were also light and all to open ports with\n",
            "  June/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\n",
            "  under New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\n",
            "  per tonne FOB.\n",
            "      Routine sales of butter were made. March/April sold at\n",
            "  4,340, 4,345 and 4,350 dlrs.\n",
            "      April/May butter went at 2.27 times New York May, June/July\n",
            "  at 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\n",
            "  2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\n",
            "  2.27 times New York Dec, Comissaria Smith said.\n",
            "      Destinations were the U.S., Covertible currency areas,\n",
            "  Uruguay and open ports.\n",
            "      Cake sales were registered at 785 to 995 dlrs for\n",
            "  March/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\n",
            "  New York Dec for Oct/Dec.\n",
            "      Buyers were the U.S., Argentina, Uruguay and convertible\n",
            "  currency areas.\n",
            "      Liquor sales were limited with March/April selling at 2,325\n",
            "  and 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\n",
            "  York July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\n",
            "  Sept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\n",
            "  said.\n",
            "      Total Bahia sales are currently estimated at 6.13 mln bags\n",
            "  against the 1986/87 crop and 1.06 mln bags against the 1987/88\n",
            "  crop.\n",
            "      Final figures for the period to February 28 are expected to\n",
            "  be published by the Brazilian Cocoa Trade Commission after\n",
            "  carnival which ends midday on February 27.\n",
            "  \n",
            "\n",
            "\n",
            "AFTER CLEANING: bahia cocoa review shower continued throughout week bahia cocoa zone alleviating drought since early january improving prospect coming temporao although normal humidity level restored comissaria smith said weekly review dry period mean temporao late year arrival week ended february bag kilo making cumulative total season mln stage last year seems cocoa delivered earlier consignment included arrival figure comissaria smith said still doubt much old crop cocoa still available harvesting practically come end total bahia crop estimate around mln bag sale standing almost mln hundred thousand bag still hand farmer middleman exporter processor doubt much cocoa would fit export shipper experiencing dificulties obtaining bahia superior certificate view lower quality recent week farmer sold good part cocoa held consignment comissaria smith said spot bean price rose cruzados per arroba kilo bean shipper reluctant offer nearby shipment limited sale booked march shipment dlrs per tonne port named new crop sale also light open port junejuly going dlrs dlrs new york july augsept dlrs per tonne fob routine sale butter made marchapril sold dlrs aprilmay butter went time new york may junejuly dlrs augsept dlrs time new york sept octdec dlrs time new york dec comissaria smith said destination u covertible currency area uruguay open port cake sale registered dlrs marchapril dlrs may dlrs aug time new york dec octdec buyer u argentina uruguay convertible currency area liquor sale limited marchapril selling dlrs junejuly dlrs time new york july augsept dlrs time new york sept octdec time new york dec comissaria smith said total bahia sale currently estimated mln bag crop mln bag crop final figure period february expected published brazilian cocoa trade commission carnival end midday february\n"
          ]
        }
      ],
      "source": [
        "preprocessed_text_train = [preprocessingText(text) for text in text_train]\n",
        "preprocessed_text_test = [preprocessingText(text) for text in text_test]\n",
        "\n",
        "print('BEFORE CLEANING: {}'.format(text_train[0]))\n",
        "print('AFTER CLEANING: {}'.format(preprocessed_text_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6CbF8ci4yW2"
      },
      "source": [
        "##Text to Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g8rCpqQC4OHq"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_text_train)\n",
        "\n",
        "sequences_text_train = tokenizer.texts_to_sequences(preprocessed_text_train)\n",
        "sequences_text_test = tokenizer.texts_to_sequences(preprocessed_text_test)\n",
        "\n",
        "X_train = torch.from_numpy(pad_sequences(sequences_text_train, maxlen=128))\n",
        "X_test = torch.from_numpy(pad_sequences(sequences_text_test, maxlen=128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZi_UZbg43uw"
      },
      "source": [
        "## Build Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7_sCNk3vQvD",
        "outputId": "fc2014cd-eeb7-4dfe-d8e5-2ae4cbac5872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VOCAB_SIZE : 25306\n",
            "TOTAL OF UNKNOWN WORD : 13311\n"
          ]
        }
      ],
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = torch.zeros(VOCAB_SIZE, 300)\n",
        "\n",
        "unk = 0\n",
        "for i in range(1, VOCAB_SIZE):\n",
        "  word = tokenizer.index_word[i]\n",
        "  if word in WRVModel.keys():\n",
        "    tensor = torch.from_numpy(WRVModel[word]).float()\n",
        "    if tensor.size(0)<300:\n",
        "      # Calculate the number of zeros to add\n",
        "      num_zeros = 300 - tensor.size(0)\n",
        "\n",
        "      # Pad the tensor with zeros to reach a size of 300\n",
        "      embedding_matrix[i] = torch.cat((tensor, torch.zeros(num_zeros)), dim=0)\n",
        "    elif tensor.size(0)>300:\n",
        "      embedding_matrix[i] = tensor[:300]\n",
        "    else:\n",
        "      embedding_matrix[i]= tensor\n",
        "  else:\n",
        "    unk +=1\n",
        "print('VOCAB_SIZE : {}'.format(VOCAB_SIZE))\n",
        "print('TOTAL OF UNKNOWN WORD : {}'.format(unk))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZHdtuWH6kiT"
      },
      "source": [
        "#Preparing Graph Attention Networks Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhxTcQ_u6sUc"
      },
      "source": [
        "## Label Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URGNwS1hP_fr",
        "outputId": "a25ad869-8619-4887-c2e7-4421d3189a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3568, -0.1348,  0.0790,  ..., -0.0384,  0.2948,  0.1996],\n",
            "        ...,\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.5990, -0.3234, -0.2749,  ...,  0.6343,  0.5300,  0.0299],\n",
            "        [-0.4541, -0.1300, -0.5178,  ..., -1.1637, -0.2056, -0.3177]])\n"
          ]
        }
      ],
      "source": [
        "label_embedding = torch.zeros(90,300)\n",
        "\n",
        "for index, label in enumerate(mlb.classes_):\n",
        "  words = label.split('-')\n",
        "  num_of_words = len(words)\n",
        "\n",
        "  for sublabel in words:\n",
        "    if sublabel in WRVModel.keys():\n",
        "      label_embedding[index] +=  torch.from_numpy(WRVModel[sublabel])\n",
        "  label_embedding[index] = label_embedding[index]/num_of_words\n",
        "\n",
        "print(label_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PvT83CK66F-"
      },
      "source": [
        "##Adjacency Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhYLvfvTtZOj",
        "outputId": "8cfb0a5f-5a34-41c2-8850-c6fa18126076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0012],\n",
            "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0286],\n",
            "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0000, 0.0000],\n",
            "        [0.0952, 0.0476, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])\n"
          ]
        }
      ],
      "source": [
        "adjacency = buildAdjacencyCOOC(y_train.numpy())\n",
        "print(adjacency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0dSc09x7lGm"
      },
      "source": [
        "#Preparing DataLoader and Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uoQlm38OS5S"
      },
      "source": [
        "## Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2vePGeWyvocZ"
      },
      "outputs": [],
      "source": [
        "class dataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x  = x\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi7g_VSYOYny"
      },
      "source": [
        "##Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "F1Vbv5m3e9bW",
        "outputId": "44346bdd-f600-4e81-bfc6-ff6d4efb0211"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b85657e-85e2-4a4e-8505-6bf421ed5f37\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b85657e-85e2-4a4e-8505-6bf421ed5f37\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving models_graphsage.py to models_graphsage (5).py\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('models_graphsage.py','wb').write(src)\n",
        "import models_graphsage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T_ve2FfBfng_"
      },
      "outputs": [],
      "source": [
        "from models_graphsage import MAGNET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "apRrzy6cWzN5"
      },
      "outputs": [],
      "source": [
        "model = MAGNET(300, 250, adjacency, embedding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvY8vULwSbXB"
      },
      "source": [
        "#Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTh-66uJSnij"
      },
      "source": [
        "##Configure Save PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nC2_KVL5SfG-"
      },
      "outputs": [],
      "source": [
        "save_path = './train_result.pt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwv_OW8zSx7W"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XeYrWVgvYrV",
        "outputId": "fc796d09-6a96-40d6-82d5-ccf281c44bf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:1 loss:3.39327 hamming_loss:0.02995 micro_f1score:0.04033 val_hamming_loss:0.01219 val_micro_f1score:0.22792\n",
            "epoch:2 loss:1.58545 hamming_loss:0.01204 micro_f1score:0.28118 val_hamming_loss:0.01069 val_micro_f1score:0.40214\n",
            "epoch:3 loss:1.38908 hamming_loss:0.01079 micro_f1score:0.38349 val_hamming_loss:0.01040 val_micro_f1score:0.43024\n",
            "epoch:4 loss:1.29980 hamming_loss:0.01027 micro_f1score:0.40925 val_hamming_loss:0.01005 val_micro_f1score:0.43054\n",
            "epoch:5 loss:1.29742 hamming_loss:0.01064 micro_f1score:0.37639 val_hamming_loss:0.01024 val_micro_f1score:0.41134\n",
            "epoch:6 loss:1.20651 hamming_loss:0.01046 micro_f1score:0.40141 val_hamming_loss:0.01010 val_micro_f1score:0.43268\n",
            "epoch:7 loss:1.05118 hamming_loss:0.00936 micro_f1score:0.51737 val_hamming_loss:0.00771 val_micro_f1score:0.62248\n",
            "epoch:8 loss:1.10468 hamming_loss:0.01002 micro_f1score:0.47247 val_hamming_loss:0.00880 val_micro_f1score:0.53726\n",
            "epoch:9 loss:1.02059 hamming_loss:0.00913 micro_f1score:0.53971 val_hamming_loss:0.00772 val_micro_f1score:0.62629\n",
            "epoch:10 loss:0.92231 hamming_loss:0.00822 micro_f1score:0.60853 val_hamming_loss:0.00757 val_micro_f1score:0.64094\n",
            "epoch:11 loss:0.88559 hamming_loss:0.00799 micro_f1score:0.62019 val_hamming_loss:0.00738 val_micro_f1score:0.66320\n",
            "epoch:12 loss:0.88973 hamming_loss:0.00799 micro_f1score:0.62708 val_hamming_loss:0.00735 val_micro_f1score:0.65080\n",
            "epoch:13 loss:0.82473 hamming_loss:0.00753 micro_f1score:0.65158 val_hamming_loss:0.00712 val_micro_f1score:0.66412\n",
            "epoch:14 loss:0.78876 hamming_loss:0.00729 micro_f1score:0.66447 val_hamming_loss:0.00690 val_micro_f1score:0.68332\n",
            "epoch:15 loss:0.78732 hamming_loss:0.00739 micro_f1score:0.65973 val_hamming_loss:0.00693 val_micro_f1score:0.67674\n",
            "epoch:16 loss:0.73851 hamming_loss:0.00704 micro_f1score:0.68072 val_hamming_loss:0.00681 val_micro_f1score:0.70364\n",
            "epoch:17 loss:0.74182 hamming_loss:0.00706 micro_f1score:0.68376 val_hamming_loss:0.00644 val_micro_f1score:0.71508\n",
            "epoch:18 loss:0.71233 hamming_loss:0.00692 micro_f1score:0.68939 val_hamming_loss:0.00635 val_micro_f1score:0.71673\n",
            "epoch:19 loss:0.68088 hamming_loss:0.00668 micro_f1score:0.70585 val_hamming_loss:0.00610 val_micro_f1score:0.73209\n",
            "epoch:20 loss:0.65697 hamming_loss:0.00650 micro_f1score:0.71505 val_hamming_loss:0.00602 val_micro_f1score:0.73887\n",
            "epoch:21 loss:0.64102 hamming_loss:0.00632 micro_f1score:0.72612 val_hamming_loss:0.00605 val_micro_f1score:0.73716\n",
            "epoch:22 loss:0.61970 hamming_loss:0.00629 micro_f1score:0.72648 val_hamming_loss:0.00597 val_micro_f1score:0.74628\n",
            "epoch:23 loss:0.59533 hamming_loss:0.00608 micro_f1score:0.73854 val_hamming_loss:0.00593 val_micro_f1score:0.75112\n",
            "epoch:24 loss:0.58804 hamming_loss:0.00596 micro_f1score:0.74575 val_hamming_loss:0.00587 val_micro_f1score:0.75627\n",
            "epoch:25 loss:0.60505 hamming_loss:0.00625 micro_f1score:0.73209 val_hamming_loss:0.00605 val_micro_f1score:0.74224\n",
            "epoch:26 loss:0.58090 hamming_loss:0.00594 micro_f1score:0.74417 val_hamming_loss:0.00586 val_micro_f1score:0.75564\n",
            "epoch:27 loss:0.54760 hamming_loss:0.00567 micro_f1score:0.75916 val_hamming_loss:0.00578 val_micro_f1score:0.75960\n",
            "epoch:28 loss:0.53365 hamming_loss:0.00564 micro_f1score:0.76169 val_hamming_loss:0.00563 val_micro_f1score:0.76627\n",
            "epoch:29 loss:0.52893 hamming_loss:0.00557 micro_f1score:0.76623 val_hamming_loss:0.00550 val_micro_f1score:0.76714\n",
            "epoch:30 loss:0.51122 hamming_loss:0.00545 micro_f1score:0.77242 val_hamming_loss:0.00568 val_micro_f1score:0.76472\n",
            "epoch:31 loss:0.50308 hamming_loss:0.00533 micro_f1score:0.77773 val_hamming_loss:0.00548 val_micro_f1score:0.76947\n",
            "epoch:32 loss:0.49926 hamming_loss:0.00537 micro_f1score:0.77660 val_hamming_loss:0.00569 val_micro_f1score:0.76571\n",
            "epoch:33 loss:0.60157 hamming_loss:0.00609 micro_f1score:0.74248 val_hamming_loss:0.00558 val_micro_f1score:0.75994\n",
            "epoch:34 loss:0.54265 hamming_loss:0.00563 micro_f1score:0.76136 val_hamming_loss:0.00568 val_micro_f1score:0.76246\n",
            "epoch:35 loss:0.52340 hamming_loss:0.00560 micro_f1score:0.76326 val_hamming_loss:0.00547 val_micro_f1score:0.77320\n",
            "epoch:36 loss:0.50294 hamming_loss:0.00532 micro_f1score:0.77774 val_hamming_loss:0.00555 val_micro_f1score:0.76675\n",
            "epoch:37 loss:0.48308 hamming_loss:0.00520 micro_f1score:0.78474 val_hamming_loss:0.00541 val_micro_f1score:0.77816\n",
            "epoch:38 loss:0.48512 hamming_loss:0.00517 micro_f1score:0.78589 val_hamming_loss:0.00551 val_micro_f1score:0.76715\n",
            "epoch:39 loss:0.48582 hamming_loss:0.00519 micro_f1score:0.78355 val_hamming_loss:0.00550 val_micro_f1score:0.77425\n",
            "epoch:40 loss:0.45795 hamming_loss:0.00501 micro_f1score:0.79405 val_hamming_loss:0.00536 val_micro_f1score:0.77845\n",
            "epoch:41 loss:0.44648 hamming_loss:0.00493 micro_f1score:0.79713 val_hamming_loss:0.00526 val_micro_f1score:0.78306\n",
            "epoch:42 loss:0.44663 hamming_loss:0.00495 micro_f1score:0.79756 val_hamming_loss:0.00532 val_micro_f1score:0.78347\n",
            "epoch:43 loss:0.43470 hamming_loss:0.00481 micro_f1score:0.80378 val_hamming_loss:0.00522 val_micro_f1score:0.78759\n",
            "epoch:44 loss:0.41185 hamming_loss:0.00457 micro_f1score:0.81484 val_hamming_loss:0.00520 val_micro_f1score:0.79020\n",
            "epoch:45 loss:0.42061 hamming_loss:0.00462 micro_f1score:0.81256 val_hamming_loss:0.00513 val_micro_f1score:0.79506\n",
            "epoch:46 loss:0.39851 hamming_loss:0.00456 micro_f1score:0.81680 val_hamming_loss:0.00507 val_micro_f1score:0.79439\n",
            "epoch:47 loss:0.39232 hamming_loss:0.00439 micro_f1score:0.82310 val_hamming_loss:0.00505 val_micro_f1score:0.79578\n",
            "epoch:48 loss:0.37520 hamming_loss:0.00432 micro_f1score:0.82819 val_hamming_loss:0.00488 val_micro_f1score:0.80256\n",
            "epoch:49 loss:0.37279 hamming_loss:0.00427 micro_f1score:0.82953 val_hamming_loss:0.00487 val_micro_f1score:0.80339\n",
            "epoch:50 loss:0.35368 hamming_loss:0.00415 micro_f1score:0.83492 val_hamming_loss:0.00491 val_micro_f1score:0.80243\n",
            "epoch:51 loss:0.35047 hamming_loss:0.00407 micro_f1score:0.83859 val_hamming_loss:0.00480 val_micro_f1score:0.80675\n",
            "epoch:52 loss:0.34333 hamming_loss:0.00400 micro_f1score:0.84260 val_hamming_loss:0.00482 val_micro_f1score:0.80448\n",
            "epoch:53 loss:0.33352 hamming_loss:0.00392 micro_f1score:0.84537 val_hamming_loss:0.00475 val_micro_f1score:0.80746\n",
            "epoch:54 loss:0.32458 hamming_loss:0.00385 micro_f1score:0.84875 val_hamming_loss:0.00478 val_micro_f1score:0.80666\n",
            "epoch:55 loss:0.32510 hamming_loss:0.00383 micro_f1score:0.84996 val_hamming_loss:0.00482 val_micro_f1score:0.80372\n",
            "epoch:56 loss:0.32679 hamming_loss:0.00381 micro_f1score:0.85094 val_hamming_loss:0.00481 val_micro_f1score:0.80519\n",
            "epoch:57 loss:0.31482 hamming_loss:0.00373 micro_f1score:0.85401 val_hamming_loss:0.00486 val_micro_f1score:0.80803\n",
            "epoch:58 loss:0.31442 hamming_loss:0.00374 micro_f1score:0.85418 val_hamming_loss:0.00477 val_micro_f1score:0.81046\n",
            "epoch:59 loss:0.29788 hamming_loss:0.00353 micro_f1score:0.86261 val_hamming_loss:0.00470 val_micro_f1score:0.81316\n",
            "epoch:60 loss:0.29405 hamming_loss:0.00352 micro_f1score:0.86376 val_hamming_loss:0.00469 val_micro_f1score:0.81546\n",
            "epoch:61 loss:0.29186 hamming_loss:0.00348 micro_f1score:0.86492 val_hamming_loss:0.00467 val_micro_f1score:0.81584\n",
            "epoch:62 loss:0.28645 hamming_loss:0.00338 micro_f1score:0.86959 val_hamming_loss:0.00471 val_micro_f1score:0.81071\n",
            "epoch:63 loss:0.27799 hamming_loss:0.00343 micro_f1score:0.86790 val_hamming_loss:0.00476 val_micro_f1score:0.80950\n",
            "epoch:64 loss:0.28289 hamming_loss:0.00339 micro_f1score:0.86946 val_hamming_loss:0.00474 val_micro_f1score:0.81115\n",
            "epoch:65 loss:0.29064 hamming_loss:0.00346 micro_f1score:0.86695 val_hamming_loss:0.00474 val_micro_f1score:0.81263\n",
            "epoch:66 loss:0.27792 hamming_loss:0.00340 micro_f1score:0.86943 val_hamming_loss:0.00481 val_micro_f1score:0.81105\n",
            "epoch:67 loss:0.26636 hamming_loss:0.00321 micro_f1score:0.87663 val_hamming_loss:0.00461 val_micro_f1score:0.81808\n",
            "epoch:68 loss:0.25861 hamming_loss:0.00322 micro_f1score:0.87696 val_hamming_loss:0.00466 val_micro_f1score:0.81897\n",
            "epoch:69 loss:0.25541 hamming_loss:0.00309 micro_f1score:0.88188 val_hamming_loss:0.00455 val_micro_f1score:0.82223\n",
            "epoch:70 loss:0.24578 hamming_loss:0.00303 micro_f1score:0.88467 val_hamming_loss:0.00469 val_micro_f1score:0.81934\n",
            "epoch:71 loss:0.25042 hamming_loss:0.00307 micro_f1score:0.88278 val_hamming_loss:0.00463 val_micro_f1score:0.81775\n",
            "epoch:72 loss:0.24221 hamming_loss:0.00298 micro_f1score:0.88648 val_hamming_loss:0.00462 val_micro_f1score:0.81978\n",
            "epoch:73 loss:0.23841 hamming_loss:0.00295 micro_f1score:0.88774 val_hamming_loss:0.00487 val_micro_f1score:0.80750\n",
            "epoch:74 loss:0.24099 hamming_loss:0.00298 micro_f1score:0.88703 val_hamming_loss:0.00464 val_micro_f1score:0.81776\n",
            "epoch:75 loss:0.23983 hamming_loss:0.00285 micro_f1score:0.89203 val_hamming_loss:0.00445 val_micro_f1score:0.82293\n",
            "epoch:76 loss:0.24093 hamming_loss:0.00286 micro_f1score:0.89140 val_hamming_loss:0.00462 val_micro_f1score:0.81834\n",
            "epoch:77 loss:0.22465 hamming_loss:0.00277 micro_f1score:0.89532 val_hamming_loss:0.00436 val_micro_f1score:0.82880\n",
            "epoch:78 loss:0.21814 hamming_loss:0.00272 micro_f1score:0.89741 val_hamming_loss:0.00433 val_micro_f1score:0.83157\n",
            "epoch:79 loss:0.21195 hamming_loss:0.00258 micro_f1score:0.90255 val_hamming_loss:0.00440 val_micro_f1score:0.82818\n",
            "epoch:80 loss:0.21036 hamming_loss:0.00268 micro_f1score:0.89886 val_hamming_loss:0.00439 val_micro_f1score:0.82869\n",
            "epoch:81 loss:0.20176 hamming_loss:0.00242 micro_f1score:0.90948 val_hamming_loss:0.00429 val_micro_f1score:0.83383\n",
            "epoch:82 loss:0.19787 hamming_loss:0.00236 micro_f1score:0.91164 val_hamming_loss:0.00430 val_micro_f1score:0.83209\n",
            "epoch:83 loss:0.19276 hamming_loss:0.00236 micro_f1score:0.91180 val_hamming_loss:0.00414 val_micro_f1score:0.83935\n",
            "epoch:84 loss:0.18910 hamming_loss:0.00239 micro_f1score:0.91080 val_hamming_loss:0.00420 val_micro_f1score:0.83531\n",
            "epoch:85 loss:0.18391 hamming_loss:0.00228 micro_f1score:0.91480 val_hamming_loss:0.00422 val_micro_f1score:0.83631\n",
            "epoch:86 loss:0.18389 hamming_loss:0.00223 micro_f1score:0.91679 val_hamming_loss:0.00415 val_micro_f1score:0.83655\n",
            "epoch:87 loss:0.18040 hamming_loss:0.00224 micro_f1score:0.91678 val_hamming_loss:0.00406 val_micro_f1score:0.84055\n",
            "epoch:88 loss:0.18357 hamming_loss:0.00222 micro_f1score:0.91717 val_hamming_loss:0.00402 val_micro_f1score:0.84291\n",
            "epoch:89 loss:0.17471 hamming_loss:0.00211 micro_f1score:0.92131 val_hamming_loss:0.00405 val_micro_f1score:0.84168\n",
            "epoch:90 loss:0.17017 hamming_loss:0.00207 micro_f1score:0.92287 val_hamming_loss:0.00421 val_micro_f1score:0.83487\n",
            "epoch:91 loss:0.17203 hamming_loss:0.00203 micro_f1score:0.92463 val_hamming_loss:0.00427 val_micro_f1score:0.83259\n",
            "epoch:92 loss:0.18334 hamming_loss:0.00228 micro_f1score:0.91509 val_hamming_loss:0.00411 val_micro_f1score:0.83816\n",
            "epoch:93 loss:0.18425 hamming_loss:0.00219 micro_f1score:0.91819 val_hamming_loss:0.00417 val_micro_f1score:0.83632\n",
            "epoch:94 loss:0.16247 hamming_loss:0.00195 micro_f1score:0.92759 val_hamming_loss:0.00396 val_micro_f1score:0.84478\n",
            "epoch:95 loss:0.15845 hamming_loss:0.00200 micro_f1score:0.92561 val_hamming_loss:0.00410 val_micro_f1score:0.83805\n",
            "epoch:96 loss:0.15842 hamming_loss:0.00192 micro_f1score:0.92877 val_hamming_loss:0.00412 val_micro_f1score:0.83902\n",
            "epoch:97 loss:0.15701 hamming_loss:0.00192 micro_f1score:0.92878 val_hamming_loss:0.00397 val_micro_f1score:0.84426\n",
            "epoch:98 loss:0.16169 hamming_loss:0.00189 micro_f1score:0.92970 val_hamming_loss:0.00400 val_micro_f1score:0.84218\n",
            "epoch:99 loss:0.15079 hamming_loss:0.00185 micro_f1score:0.93126 val_hamming_loss:0.00390 val_micro_f1score:0.84663\n",
            "epoch:100 loss:0.14960 hamming_loss:0.00180 micro_f1score:0.93332 val_hamming_loss:0.00402 val_micro_f1score:0.84103\n",
            "epoch:101 loss:0.15262 hamming_loss:0.00181 micro_f1score:0.93293 val_hamming_loss:0.00397 val_micro_f1score:0.84395\n",
            "epoch:102 loss:0.15335 hamming_loss:0.00186 micro_f1score:0.93110 val_hamming_loss:0.00398 val_micro_f1score:0.84331\n",
            "epoch:103 loss:0.15262 hamming_loss:0.00180 micro_f1score:0.93330 val_hamming_loss:0.00399 val_micro_f1score:0.84150\n",
            "epoch:104 loss:0.14422 hamming_loss:0.00168 micro_f1score:0.93766 val_hamming_loss:0.00379 val_micro_f1score:0.85197\n",
            "epoch:105 loss:0.13947 hamming_loss:0.00171 micro_f1score:0.93672 val_hamming_loss:0.00406 val_micro_f1score:0.84071\n",
            "epoch:106 loss:0.14702 hamming_loss:0.00178 micro_f1score:0.93410 val_hamming_loss:0.00382 val_micro_f1score:0.85159\n",
            "epoch:107 loss:0.14276 hamming_loss:0.00171 micro_f1score:0.93706 val_hamming_loss:0.00396 val_micro_f1score:0.84382\n",
            "epoch:108 loss:0.15172 hamming_loss:0.00177 micro_f1score:0.93427 val_hamming_loss:0.00393 val_micro_f1score:0.84599\n",
            "epoch:109 loss:0.13963 hamming_loss:0.00171 micro_f1score:0.93684 val_hamming_loss:0.00393 val_micro_f1score:0.84535\n",
            "epoch:110 loss:0.12789 hamming_loss:0.00150 micro_f1score:0.94454 val_hamming_loss:0.00387 val_micro_f1score:0.84880\n",
            "epoch:111 loss:0.12216 hamming_loss:0.00148 micro_f1score:0.94548 val_hamming_loss:0.00386 val_micro_f1score:0.84943\n",
            "epoch:112 loss:0.12371 hamming_loss:0.00146 micro_f1score:0.94624 val_hamming_loss:0.00389 val_micro_f1score:0.84842\n",
            "epoch:113 loss:0.11973 hamming_loss:0.00140 micro_f1score:0.94852 val_hamming_loss:0.00381 val_micro_f1score:0.85216\n",
            "epoch:114 loss:0.12184 hamming_loss:0.00143 micro_f1score:0.94736 val_hamming_loss:0.00386 val_micro_f1score:0.84957\n",
            "epoch:115 loss:0.12843 hamming_loss:0.00152 micro_f1score:0.94406 val_hamming_loss:0.00379 val_micro_f1score:0.85218\n",
            "epoch:116 loss:0.11441 hamming_loss:0.00135 micro_f1score:0.95015 val_hamming_loss:0.00384 val_micro_f1score:0.85059\n",
            "epoch:117 loss:0.11818 hamming_loss:0.00142 micro_f1score:0.94759 val_hamming_loss:0.00389 val_micro_f1score:0.84940\n",
            "epoch:118 loss:0.11585 hamming_loss:0.00140 micro_f1score:0.94865 val_hamming_loss:0.00383 val_micro_f1score:0.85092\n",
            "epoch:119 loss:0.11847 hamming_loss:0.00131 micro_f1score:0.95184 val_hamming_loss:0.00388 val_micro_f1score:0.85074\n",
            "epoch:120 loss:0.23130 hamming_loss:0.00239 micro_f1score:0.91090 val_hamming_loss:0.00406 val_micro_f1score:0.84247\n",
            "epoch:121 loss:0.17193 hamming_loss:0.00192 micro_f1score:0.92865 val_hamming_loss:0.00386 val_micro_f1score:0.84951\n",
            "epoch:122 loss:0.14761 hamming_loss:0.00173 micro_f1score:0.93603 val_hamming_loss:0.00384 val_micro_f1score:0.85157\n",
            "epoch:123 loss:0.13002 hamming_loss:0.00155 micro_f1score:0.94266 val_hamming_loss:0.00379 val_micro_f1score:0.85274\n",
            "epoch:124 loss:0.11835 hamming_loss:0.00138 micro_f1score:0.94932 val_hamming_loss:0.00386 val_micro_f1score:0.84948\n",
            "epoch:125 loss:0.12150 hamming_loss:0.00141 micro_f1score:0.94812 val_hamming_loss:0.00385 val_micro_f1score:0.85104\n",
            "epoch:126 loss:0.11732 hamming_loss:0.00141 micro_f1score:0.94806 val_hamming_loss:0.00384 val_micro_f1score:0.85191\n",
            "epoch:127 loss:0.11155 hamming_loss:0.00134 micro_f1score:0.95053 val_hamming_loss:0.00376 val_micro_f1score:0.85479\n",
            "epoch:128 loss:0.11332 hamming_loss:0.00134 micro_f1score:0.95072 val_hamming_loss:0.00372 val_micro_f1score:0.85605\n",
            "epoch:129 loss:0.11130 hamming_loss:0.00131 micro_f1score:0.95192 val_hamming_loss:0.00383 val_micro_f1score:0.85328\n",
            "epoch:130 loss:0.11247 hamming_loss:0.00135 micro_f1score:0.95031 val_hamming_loss:0.00380 val_micro_f1score:0.85424\n",
            "epoch:131 loss:0.11513 hamming_loss:0.00133 micro_f1score:0.95094 val_hamming_loss:0.00377 val_micro_f1score:0.85475\n",
            "epoch:132 loss:0.10995 hamming_loss:0.00132 micro_f1score:0.95140 val_hamming_loss:0.00370 val_micro_f1score:0.85665\n",
            "epoch:133 loss:0.11351 hamming_loss:0.00136 micro_f1score:0.95011 val_hamming_loss:0.00372 val_micro_f1score:0.85617\n",
            "epoch:134 loss:0.10022 hamming_loss:0.00115 micro_f1score:0.95752 val_hamming_loss:0.00373 val_micro_f1score:0.85629\n",
            "epoch:135 loss:0.09866 hamming_loss:0.00118 micro_f1score:0.95676 val_hamming_loss:0.00365 val_micro_f1score:0.85841\n",
            "epoch:136 loss:0.09325 hamming_loss:0.00111 micro_f1score:0.95927 val_hamming_loss:0.00372 val_micro_f1score:0.85690\n",
            "epoch:137 loss:0.09688 hamming_loss:0.00121 micro_f1score:0.95562 val_hamming_loss:0.00370 val_micro_f1score:0.85649\n",
            "epoch:138 loss:0.09638 hamming_loss:0.00114 micro_f1score:0.95826 val_hamming_loss:0.00377 val_micro_f1score:0.85471\n",
            "epoch:139 loss:0.09689 hamming_loss:0.00115 micro_f1score:0.95790 val_hamming_loss:0.00373 val_micro_f1score:0.85596\n",
            "epoch:140 loss:0.08868 hamming_loss:0.00105 micro_f1score:0.96156 val_hamming_loss:0.00370 val_micro_f1score:0.85706\n",
            "epoch:141 loss:0.08883 hamming_loss:0.00107 micro_f1score:0.96072 val_hamming_loss:0.00366 val_micro_f1score:0.85869\n",
            "epoch:142 loss:0.09047 hamming_loss:0.00108 micro_f1score:0.96029 val_hamming_loss:0.00374 val_micro_f1score:0.85474\n",
            "epoch:143 loss:0.09188 hamming_loss:0.00109 micro_f1score:0.96022 val_hamming_loss:0.00379 val_micro_f1score:0.85324\n",
            "epoch:144 loss:0.10103 hamming_loss:0.00118 micro_f1score:0.95677 val_hamming_loss:0.00371 val_micro_f1score:0.85629\n",
            "epoch:145 loss:0.09844 hamming_loss:0.00114 micro_f1score:0.95821 val_hamming_loss:0.00374 val_micro_f1score:0.85572\n",
            "epoch:146 loss:0.09929 hamming_loss:0.00114 micro_f1score:0.95829 val_hamming_loss:0.00371 val_micro_f1score:0.85690\n",
            "epoch:147 loss:0.08680 hamming_loss:0.00103 micro_f1score:0.96221 val_hamming_loss:0.00366 val_micro_f1score:0.85849\n",
            "epoch:148 loss:0.08888 hamming_loss:0.00104 micro_f1score:0.96206 val_hamming_loss:0.00377 val_micro_f1score:0.85455\n",
            "epoch:149 loss:0.08755 hamming_loss:0.00101 micro_f1score:0.96318 val_hamming_loss:0.00380 val_micro_f1score:0.85341\n",
            "epoch:150 loss:0.08165 hamming_loss:0.00095 micro_f1score:0.96524 val_hamming_loss:0.00368 val_micro_f1score:0.85706\n",
            "epoch:151 loss:0.08310 hamming_loss:0.00097 micro_f1score:0.96446 val_hamming_loss:0.00378 val_micro_f1score:0.85414\n",
            "epoch:152 loss:0.08452 hamming_loss:0.00094 micro_f1score:0.96573 val_hamming_loss:0.00386 val_micro_f1score:0.85194\n",
            "epoch:153 loss:0.08503 hamming_loss:0.00096 micro_f1score:0.96471 val_hamming_loss:0.00384 val_micro_f1score:0.85140\n",
            "epoch:154 loss:0.08485 hamming_loss:0.00101 micro_f1score:0.96306 val_hamming_loss:0.00363 val_micro_f1score:0.86022\n",
            "epoch:155 loss:0.08053 hamming_loss:0.00096 micro_f1score:0.96485 val_hamming_loss:0.00367 val_micro_f1score:0.85796\n",
            "epoch:156 loss:0.08079 hamming_loss:0.00091 micro_f1score:0.96656 val_hamming_loss:0.00365 val_micro_f1score:0.85849\n",
            "epoch:157 loss:0.08998 hamming_loss:0.00109 micro_f1score:0.96031 val_hamming_loss:0.00379 val_micro_f1score:0.85240\n",
            "epoch:158 loss:0.08645 hamming_loss:0.00100 micro_f1score:0.96340 val_hamming_loss:0.00358 val_micro_f1score:0.86201\n",
            "epoch:159 loss:0.08121 hamming_loss:0.00095 micro_f1score:0.96533 val_hamming_loss:0.00364 val_micro_f1score:0.85942\n",
            "epoch:160 loss:0.07936 hamming_loss:0.00091 micro_f1score:0.96664 val_hamming_loss:0.00372 val_micro_f1score:0.85637\n",
            "epoch:161 loss:0.08028 hamming_loss:0.00095 micro_f1score:0.96524 val_hamming_loss:0.00373 val_micro_f1score:0.85551\n",
            "epoch:162 loss:0.08351 hamming_loss:0.00100 micro_f1score:0.96336 val_hamming_loss:0.00372 val_micro_f1score:0.85706\n",
            "epoch:163 loss:0.08152 hamming_loss:0.00096 micro_f1score:0.96478 val_hamming_loss:0.00375 val_micro_f1score:0.85515\n",
            "epoch:164 loss:0.08763 hamming_loss:0.00098 micro_f1score:0.96404 val_hamming_loss:0.00371 val_micro_f1score:0.85649\n",
            "epoch:165 loss:0.07790 hamming_loss:0.00091 micro_f1score:0.96673 val_hamming_loss:0.00373 val_micro_f1score:0.85682\n",
            "epoch:166 loss:0.07823 hamming_loss:0.00093 micro_f1score:0.96609 val_hamming_loss:0.00364 val_micro_f1score:0.86002\n",
            "epoch:167 loss:0.07641 hamming_loss:0.00086 micro_f1score:0.96855 val_hamming_loss:0.00379 val_micro_f1score:0.85244\n",
            "epoch:168 loss:0.07505 hamming_loss:0.00086 micro_f1score:0.96850 val_hamming_loss:0.00369 val_micro_f1score:0.85763\n",
            "epoch:169 loss:0.07852 hamming_loss:0.00095 micro_f1score:0.96537 val_hamming_loss:0.00373 val_micro_f1score:0.85666\n",
            "epoch:170 loss:0.07868 hamming_loss:0.00093 micro_f1score:0.96618 val_hamming_loss:0.00372 val_micro_f1score:0.85633\n",
            "epoch:171 loss:0.07266 hamming_loss:0.00082 micro_f1score:0.96997 val_hamming_loss:0.00369 val_micro_f1score:0.85743\n",
            "epoch:172 loss:0.07421 hamming_loss:0.00087 micro_f1score:0.96813 val_hamming_loss:0.00385 val_micro_f1score:0.85172\n",
            "epoch:173 loss:0.08078 hamming_loss:0.00091 micro_f1score:0.96664 val_hamming_loss:0.00389 val_micro_f1score:0.85089\n",
            "epoch:174 loss:0.07765 hamming_loss:0.00084 micro_f1score:0.96919 val_hamming_loss:0.00385 val_micro_f1score:0.85125\n",
            "epoch:175 loss:0.08359 hamming_loss:0.00099 micro_f1score:0.96361 val_hamming_loss:0.00381 val_micro_f1score:0.85354\n",
            "epoch:176 loss:0.07938 hamming_loss:0.00091 micro_f1score:0.96676 val_hamming_loss:0.00375 val_micro_f1score:0.85540\n",
            "epoch:177 loss:0.07477 hamming_loss:0.00088 micro_f1score:0.96801 val_hamming_loss:0.00372 val_micro_f1score:0.85714\n",
            "epoch:178 loss:0.07322 hamming_loss:0.00086 micro_f1score:0.96869 val_hamming_loss:0.00377 val_micro_f1score:0.85585\n",
            "epoch:179 loss:0.06888 hamming_loss:0.00078 micro_f1score:0.97133 val_hamming_loss:0.00379 val_micro_f1score:0.85403\n",
            "epoch:180 loss:0.06949 hamming_loss:0.00080 micro_f1score:0.97088 val_hamming_loss:0.00374 val_micro_f1score:0.85662\n",
            "epoch:181 loss:0.07965 hamming_loss:0.00091 micro_f1score:0.96676 val_hamming_loss:0.00372 val_micro_f1score:0.85543\n",
            "epoch:182 loss:0.07522 hamming_loss:0.00086 micro_f1score:0.96871 val_hamming_loss:0.00380 val_micro_f1score:0.85399\n",
            "epoch:183 loss:0.07467 hamming_loss:0.00089 micro_f1score:0.96734 val_hamming_loss:0.00380 val_micro_f1score:0.85283\n",
            "epoch:184 loss:0.07742 hamming_loss:0.00087 micro_f1score:0.96821 val_hamming_loss:0.00390 val_micro_f1score:0.84947\n",
            "epoch:185 loss:0.09190 hamming_loss:0.00100 micro_f1score:0.96324 val_hamming_loss:0.00377 val_micro_f1score:0.85463\n",
            "epoch:186 loss:0.08480 hamming_loss:0.00097 micro_f1score:0.96463 val_hamming_loss:0.00374 val_micro_f1score:0.85634\n",
            "epoch:187 loss:0.07577 hamming_loss:0.00086 micro_f1score:0.96872 val_hamming_loss:0.00396 val_micro_f1score:0.84848\n",
            "epoch:188 loss:0.08331 hamming_loss:0.00092 micro_f1score:0.96633 val_hamming_loss:0.00366 val_micro_f1score:0.85877\n",
            "epoch:189 loss:0.07381 hamming_loss:0.00080 micro_f1score:0.97073 val_hamming_loss:0.00368 val_micro_f1score:0.85828\n",
            "epoch:190 loss:0.07337 hamming_loss:0.00081 micro_f1score:0.97048 val_hamming_loss:0.00369 val_micro_f1score:0.85702\n",
            "epoch:191 loss:0.07213 hamming_loss:0.00086 micro_f1score:0.96847 val_hamming_loss:0.00363 val_micro_f1score:0.85998\n",
            "epoch:192 loss:0.06743 hamming_loss:0.00076 micro_f1score:0.97212 val_hamming_loss:0.00371 val_micro_f1score:0.85714\n",
            "epoch:193 loss:0.07191 hamming_loss:0.00085 micro_f1score:0.96914 val_hamming_loss:0.00363 val_micro_f1score:0.86093\n",
            "epoch:194 loss:0.06311 hamming_loss:0.00070 micro_f1score:0.97436 val_hamming_loss:0.00366 val_micro_f1score:0.85873\n",
            "epoch:195 loss:0.06919 hamming_loss:0.00079 micro_f1score:0.97115 val_hamming_loss:0.00362 val_micro_f1score:0.86082\n",
            "epoch:196 loss:0.07297 hamming_loss:0.00082 micro_f1score:0.97009 val_hamming_loss:0.00368 val_micro_f1score:0.85840\n",
            "epoch:197 loss:0.08366 hamming_loss:0.00090 micro_f1score:0.96726 val_hamming_loss:0.00384 val_micro_f1score:0.85390\n",
            "epoch:198 loss:0.07967 hamming_loss:0.00092 micro_f1score:0.96646 val_hamming_loss:0.00382 val_micro_f1score:0.85347\n",
            "epoch:199 loss:0.06916 hamming_loss:0.00079 micro_f1score:0.97122 val_hamming_loss:0.00373 val_micro_f1score:0.85641\n",
            "epoch:200 loss:0.06586 hamming_loss:0.00072 micro_f1score:0.97364 val_hamming_loss:0.00363 val_micro_f1score:0.86073\n",
            "epoch:201 loss:0.06369 hamming_loss:0.00074 micro_f1score:0.97306 val_hamming_loss:0.00369 val_micro_f1score:0.85791\n",
            "epoch:202 loss:0.06080 hamming_loss:0.00069 micro_f1score:0.97469 val_hamming_loss:0.00377 val_micro_f1score:0.85483\n",
            "epoch:203 loss:0.06333 hamming_loss:0.00073 micro_f1score:0.97352 val_hamming_loss:0.00374 val_micro_f1score:0.85568\n",
            "epoch:204 loss:0.05993 hamming_loss:0.00069 micro_f1score:0.97483 val_hamming_loss:0.00373 val_micro_f1score:0.85654\n",
            "epoch:205 loss:0.06122 hamming_loss:0.00077 micro_f1score:0.97188 val_hamming_loss:0.00367 val_micro_f1score:0.85828\n",
            "epoch:206 loss:0.06280 hamming_loss:0.00069 micro_f1score:0.97483 val_hamming_loss:0.00368 val_micro_f1score:0.85824\n",
            "epoch:207 loss:0.05918 hamming_loss:0.00071 micro_f1score:0.97405 val_hamming_loss:0.00368 val_micro_f1score:0.85880\n",
            "epoch:208 loss:0.06951 hamming_loss:0.00076 micro_f1score:0.97228 val_hamming_loss:0.00368 val_micro_f1score:0.85896\n",
            "epoch:209 loss:0.06421 hamming_loss:0.00070 micro_f1score:0.97441 val_hamming_loss:0.00372 val_micro_f1score:0.85831\n",
            "epoch:210 loss:0.06008 hamming_loss:0.00067 micro_f1score:0.97544 val_hamming_loss:0.00367 val_micro_f1score:0.85836\n",
            "epoch:211 loss:0.06248 hamming_loss:0.00072 micro_f1score:0.97391 val_hamming_loss:0.00370 val_micro_f1score:0.85682\n",
            "epoch:212 loss:0.06255 hamming_loss:0.00070 micro_f1score:0.97448 val_hamming_loss:0.00377 val_micro_f1score:0.85500\n",
            "epoch:213 loss:0.06481 hamming_loss:0.00077 micro_f1score:0.97205 val_hamming_loss:0.00378 val_micro_f1score:0.85484\n",
            "epoch:214 loss:0.06133 hamming_loss:0.00067 micro_f1score:0.97547 val_hamming_loss:0.00385 val_micro_f1score:0.85239\n",
            "epoch:215 loss:0.06606 hamming_loss:0.00073 micro_f1score:0.97319 val_hamming_loss:0.00378 val_micro_f1score:0.85385\n",
            "epoch:216 loss:0.07061 hamming_loss:0.00080 micro_f1score:0.97086 val_hamming_loss:0.00379 val_micro_f1score:0.85302\n",
            "epoch:217 loss:0.06150 hamming_loss:0.00071 micro_f1score:0.97402 val_hamming_loss:0.00367 val_micro_f1score:0.85783\n",
            "epoch:218 loss:0.06281 hamming_loss:0.00072 micro_f1score:0.97366 val_hamming_loss:0.00365 val_micro_f1score:0.86032\n",
            "epoch:219 loss:0.05867 hamming_loss:0.00066 micro_f1score:0.97582 val_hamming_loss:0.00363 val_micro_f1score:0.86026\n",
            "epoch:220 loss:0.05597 hamming_loss:0.00064 micro_f1score:0.97650 val_hamming_loss:0.00368 val_micro_f1score:0.85924\n",
            "epoch:221 loss:0.05587 hamming_loss:0.00067 micro_f1score:0.97571 val_hamming_loss:0.00365 val_micro_f1score:0.85977\n",
            "epoch:222 loss:0.05603 hamming_loss:0.00067 micro_f1score:0.97571 val_hamming_loss:0.00365 val_micro_f1score:0.86013\n",
            "epoch:223 loss:0.05527 hamming_loss:0.00064 micro_f1score:0.97664 val_hamming_loss:0.00370 val_micro_f1score:0.85839\n",
            "epoch:224 loss:0.05241 hamming_loss:0.00060 micro_f1score:0.97815 val_hamming_loss:0.00374 val_micro_f1score:0.85515\n",
            "epoch:225 loss:0.06212 hamming_loss:0.00069 micro_f1score:0.97495 val_hamming_loss:0.00375 val_micro_f1score:0.85515\n",
            "epoch:226 loss:0.05743 hamming_loss:0.00063 micro_f1score:0.97689 val_hamming_loss:0.00375 val_micro_f1score:0.85528\n",
            "epoch:227 loss:0.05384 hamming_loss:0.00062 micro_f1score:0.97747 val_hamming_loss:0.00372 val_micro_f1score:0.85742\n",
            "epoch:228 loss:0.05823 hamming_loss:0.00062 micro_f1score:0.97722 val_hamming_loss:0.00369 val_micro_f1score:0.85791\n",
            "epoch:229 loss:0.05889 hamming_loss:0.00067 micro_f1score:0.97547 val_hamming_loss:0.00372 val_micro_f1score:0.85584\n",
            "epoch:230 loss:0.05873 hamming_loss:0.00068 micro_f1score:0.97506 val_hamming_loss:0.00381 val_micro_f1score:0.85304\n",
            "epoch:231 loss:0.06488 hamming_loss:0.00072 micro_f1score:0.97374 val_hamming_loss:0.00368 val_micro_f1score:0.85804\n",
            "epoch:232 loss:0.05669 hamming_loss:0.00062 micro_f1score:0.97735 val_hamming_loss:0.00367 val_micro_f1score:0.85956\n",
            "epoch:233 loss:0.05851 hamming_loss:0.00066 micro_f1score:0.97603 val_hamming_loss:0.00374 val_micro_f1score:0.85638\n",
            "epoch:234 loss:0.06774 hamming_loss:0.00076 micro_f1score:0.97218 val_hamming_loss:0.00376 val_micro_f1score:0.85383\n",
            "epoch:235 loss:0.06786 hamming_loss:0.00072 micro_f1score:0.97374 val_hamming_loss:0.00377 val_micro_f1score:0.85396\n",
            "epoch:236 loss:0.07868 hamming_loss:0.00089 micro_f1score:0.96742 val_hamming_loss:0.00381 val_micro_f1score:0.85275\n",
            "epoch:237 loss:0.07075 hamming_loss:0.00080 micro_f1score:0.97098 val_hamming_loss:0.00380 val_micro_f1score:0.85349\n",
            "epoch:238 loss:0.06347 hamming_loss:0.00069 micro_f1score:0.97504 val_hamming_loss:0.00375 val_micro_f1score:0.85556\n",
            "epoch:239 loss:0.05875 hamming_loss:0.00067 micro_f1score:0.97559 val_hamming_loss:0.00372 val_micro_f1score:0.85621\n",
            "epoch:240 loss:0.05712 hamming_loss:0.00067 micro_f1score:0.97548 val_hamming_loss:0.00378 val_micro_f1score:0.85405\n",
            "epoch:241 loss:0.05985 hamming_loss:0.00065 micro_f1score:0.97645 val_hamming_loss:0.00387 val_micro_f1score:0.85107\n",
            "epoch:242 loss:0.05891 hamming_loss:0.00070 micro_f1score:0.97445 val_hamming_loss:0.00390 val_micro_f1score:0.84960\n",
            "epoch:243 loss:0.05996 hamming_loss:0.00069 micro_f1score:0.97496 val_hamming_loss:0.00373 val_micro_f1score:0.85666\n",
            "epoch:244 loss:0.05521 hamming_loss:0.00060 micro_f1score:0.97796 val_hamming_loss:0.00365 val_micro_f1score:0.85941\n",
            "epoch:245 loss:0.06877 hamming_loss:0.00076 micro_f1score:0.97231 val_hamming_loss:0.00380 val_micro_f1score:0.85257\n",
            "epoch:246 loss:0.06160 hamming_loss:0.00071 micro_f1score:0.97398 val_hamming_loss:0.00373 val_micro_f1score:0.85576\n",
            "epoch:247 loss:0.05263 hamming_loss:0.00060 micro_f1score:0.97800 val_hamming_loss:0.00371 val_micro_f1score:0.85653\n",
            "epoch:248 loss:0.05025 hamming_loss:0.00056 micro_f1score:0.97957 val_hamming_loss:0.00372 val_micro_f1score:0.85588\n",
            "epoch:249 loss:0.05008 hamming_loss:0.00060 micro_f1score:0.97822 val_hamming_loss:0.00379 val_micro_f1score:0.85406\n",
            "epoch:250 loss:0.05299 hamming_loss:0.00059 micro_f1score:0.97826 val_hamming_loss:0.00375 val_micro_f1score:0.85646\n"
          ]
        }
      ],
      "source": [
        "train(model, X_train, X_test, label_embedding, y_train, y_test, save_path=save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ-Lonl0f0Ih"
      },
      "outputs": [],
      "source": [
        "print(MAGNET.predict(\"dffgfgbfdbdbfn dfbngvgfhcgn\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
